{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVa3vpu6KFG"
      },
      "source": [
        "# Khmer Text-to-Speech Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nvsRMXL6Odk"
      },
      "source": [
        "## Complete Pipeline from Data Analysis to Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV65tyW16UT3"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhIRIXz76Wrh"
      },
      "source": [
        "This notebook implements a complete Khmer TTS system using NVIDIA's Tacotron2 architecture. We'll use transfer learning from a pre-trained English model to handle the low-resource nature of Khmer language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zbJt87Z7Hfd",
        "outputId": "85d4b586-eca4-42ea-fd29-224fc87d13e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../dataset\n"
          ]
        }
      ],
      "source": [
        "output_path = '../dataset'\n",
        "print(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdThKh546bDJ"
      },
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1H2YlQK6Dlr",
        "outputId": "a02ee644-8eaa-4687-91fc-72f6f5487cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import logging\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKjuM9-N64H0"
      },
      "source": [
        "## 1. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npJRTVCu6_Mz"
      },
      "source": [
        "### 1.1 Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f3NPSpEM6fz0"
      },
      "outputs": [],
      "source": [
        "def load_transcript(file_path):\n",
        "    \"\"\"Load and analyze transcript file\"\"\"\n",
        "    df = pd.read_csv(file_path,\n",
        "                    sep='\\t',\n",
        "                    header=None,\n",
        "                    names=['audio_file', 'unused', 'text'],\n",
        "                    encoding='utf-8')\n",
        "    return df[['audio_file', 'text']]\n",
        "\n",
        "# # Load dataset\n",
        "# transcript_path = f\"{output_path}/line_index.tsv\"\n",
        "# df = load_transcript(transcript_path)\n",
        "# print(f\"Total samples: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_AGGhxZ8b4m"
      },
      "source": [
        "### 1.2 Audio Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "awf11_Gj8XB8"
      },
      "outputs": [],
      "source": [
        "def analyze_audio_files(df, audio_dir):\n",
        "    \"\"\"Analyze audio file properties\"\"\"\n",
        "    durations = []\n",
        "    sample_rates = []\n",
        "    file_sizes = []\n",
        "\n",
        "    for file in tqdm(df['audio_file'], desc=\"Analyzing audio files\"):\n",
        "        path = Path(audio_dir) / f\"{file}.wav\"\n",
        "        if path.exists():\n",
        "            # Get audio info\n",
        "            info = torchaudio.info(path)\n",
        "            duration = info.num_frames / info.sample_rate\n",
        "\n",
        "            durations.append(duration)\n",
        "            sample_rates.append(info.sample_rate)\n",
        "            file_sizes.append(path.stat().st_size / 1024)  # Size in KB\n",
        "\n",
        "    return durations, sample_rates, file_sizes\n",
        "\n",
        "# Plot distributions\n",
        "def plot_audio_stats(durations, sample_rates, file_sizes):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Duration distribution\n",
        "    sns.histplot(durations, bins=50, ax=axes[0])\n",
        "    axes[0].set_title('Audio Duration Distribution')\n",
        "    axes[0].set_xlabel('Duration (seconds)')\n",
        "\n",
        "    # Sample rate distribution\n",
        "    sns.countplot(sample_rates, ax=axes[1])\n",
        "    axes[1].set_title('Sample Rate Distribution')\n",
        "\n",
        "    # File size distribution\n",
        "    sns.histplot(file_sizes, bins=50, ax=axes[2])\n",
        "    axes[2].set_title('File Size Distribution')\n",
        "    axes[2].set_xlabel('Size (KB)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Okite4o8jlo"
      },
      "source": [
        "### 1.3 Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RSkqeouW8lXC"
      },
      "outputs": [],
      "source": [
        "def analyze_text(df):\n",
        "    \"\"\"Analyze text properties\"\"\"\n",
        "    # Text length distribution\n",
        "    text_lengths = df['text'].str.len()\n",
        "\n",
        "    # Character distribution\n",
        "    char_freq = pd.Series(list(''.join(df['text']))).value_counts()\n",
        "\n",
        "    # Plot distributions\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    sns.histplot(text_lengths, bins=50, ax=ax1)\n",
        "    ax1.set_title('Text Length Distribution')\n",
        "    ax1.set_xlabel('Number of Characters')\n",
        "\n",
        "    char_freq.head(20).plot(kind='bar', ax=ax2)\n",
        "    ax2.set_title('Top 20 Character Frequencies')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return text_lengths, char_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxu8c_o8ocb"
      },
      "source": [
        "## 2. Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifd3tIDK8wUE"
      },
      "source": [
        "### 2.1 Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I6e2k5Ab8nV-"
      },
      "outputs": [],
      "source": [
        "class TextProcessor:\n",
        "    \"\"\"Text processor for NVIDIA Tacotron2 compatibility\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # NVIDIA Tacotron2 symbol set\n",
        "        self._pad = '_'\n",
        "        self._punctuation = '!\\'(),.:;? '\n",
        "        self._special = '-'\n",
        "        self._letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "        # Create symbol mappings\n",
        "        symbols = self._pad + self._special + self._punctuation + self._letters\n",
        "        self._symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
        "\n",
        "    def process_text(self, text):\n",
        "        # Romanize Khmer text\n",
        "        text = unidecode(text).lower()\n",
        "\n",
        "        # Clean text\n",
        "        text = re.sub(r'[^a-z!\\'(),.:;? -]', '', text)\n",
        "\n",
        "        # Convert to IDs\n",
        "        sequence = [self._symbol_to_id[s] for s in text if s in self._symbol_to_id]\n",
        "\n",
        "        return torch.LongTensor(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxqGwE0z86XQ"
      },
      "source": [
        "### 2.2 Audio Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nY_7SFeO80Sh"
      },
      "outputs": [],
      "source": [
        "class AudioProcessor:\n",
        "    def __init__(self, sample_rate=22050):\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def load_audio(self, file_path):\n",
        "        \"\"\"Load and preprocess audio file\"\"\"\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "\n",
        "        # Convert to mono if stereo\n",
        "        if waveform.size(0) > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Resample if necessary\n",
        "        if sr != self.sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        return waveform\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, audio_dir, text_processor, audio_processor):\n",
        "        self.df = df\n",
        "        self.audio_dir = Path(audio_dir)\n",
        "        self.text_processor = text_processor\n",
        "        self.audio_processor = audio_processor\n",
        "\n",
        "        # Add mel spectrogram transformer\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=22050,\n",
        "            n_fft=1024,\n",
        "            hop_length=256,\n",
        "            n_mels=80\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Process text\n",
        "        text = self.text_processor.process_text(row['text'])\n",
        "\n",
        "        # Load audio\n",
        "        audio_path = self.audio_dir / f\"{row['audio_file']}.wav\"\n",
        "        waveform = self.audio_processor.load_audio(audio_path)\n",
        "\n",
        "        # Generate mel spectrogram\n",
        "        mel_spec = self.mel_transform(waveform)\n",
        "        mel_spec = torch.log(torch.clamp(mel_spec, min=1e-5))\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'text_lengths': torch.LongTensor([len(text)]),\n",
        "            'mel_target': mel_spec,\n",
        "            'mel_lengths': torch.LongTensor([mel_spec.size(2)])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6ERXZD31JfhP"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to match NVIDIA Tacotron2 input format\n",
        "    \"\"\"\n",
        "    # Sort batch by text length (descending)\n",
        "    batch = sorted(batch, key=lambda x: x['text_lengths'][0], reverse=True)\n",
        "\n",
        "    # Get max lengths\n",
        "    max_text_len = max([item['text'].size(0) for item in batch])\n",
        "    max_mel_len = max([item['mel_target'].size(2) for item in batch])\n",
        "\n",
        "    # Initialize padded tensors\n",
        "    text_padded = torch.zeros(len(batch), max_text_len).long()\n",
        "    mel_padded = torch.zeros(len(batch), 80, max_mel_len)\n",
        "\n",
        "    # Fill padded tensors\n",
        "    text_lengths = []\n",
        "    mel_lengths = []\n",
        "\n",
        "    for i, item in enumerate(batch):\n",
        "        text = item['text']\n",
        "        mel = item['mel_target']\n",
        "\n",
        "        text_padded[i, :text.size(0)] = text\n",
        "        mel_padded[i, :, :mel.size(2)] = mel\n",
        "\n",
        "        text_lengths.append(item['text_lengths'][0])\n",
        "        mel_lengths.append(item['mel_lengths'][0])\n",
        "\n",
        "    # Convert lengths to tensors\n",
        "    text_lengths = torch.LongTensor(text_lengths)\n",
        "    mel_lengths = torch.LongTensor(mel_lengths)\n",
        "\n",
        "    # Return in NVIDIA Tacotron2 format: (inputs, input_lengths, targets, max_len, output_lengths)\n",
        "    return (text_padded, text_lengths, mel_padded, max_mel_len, mel_lengths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nni5jZ-L9DBj"
      },
      "source": [
        "## 3. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3cuTIz69FRs"
      },
      "source": [
        "### 3.1 Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y2qPGJSc8_bO"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_model():\n",
        "    \"\"\"Load NVIDIA Tacotron2 model\"\"\"\n",
        "    model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub',\n",
        "                          'nvidia_tacotron2',\n",
        "                          model_math='fp16',\n",
        "                          pretrained=False)\n",
        "    return model.to(device)\n",
        "\n",
        "# Load HiFi-GAN vocoder\n",
        "def load_vocoder():\n",
        "    \"\"\"Load Waveglow vocoder\"\"\"\n",
        "    try:\n",
        "        # Load Waveglow\n",
        "        waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub',\n",
        "                               'nvidia_waveglow',\n",
        "                               model_math='fp16')\n",
        "\n",
        "        # No need to call remove_weightnorm for Waveglow\n",
        "        waveglow = waveglow.to(device)\n",
        "        waveglow.eval()\n",
        "\n",
        "        return waveglow\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Waveglow: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_w-74fo9RRM"
      },
      "source": [
        "### 3.2 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DKgWSDEX9Pyd"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "import os\n",
        "\n",
        "\n",
        "def compute_metrics(mel_outputs, mel_targets, alignments, input_lengths, output_lengths):\n",
        "    \"\"\"\n",
        "    Compute various TTS evaluation metrics with proper length handling\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # For each sample in batch\n",
        "    batch_size = mel_outputs.size(0)\n",
        "    mcd_values = []\n",
        "    frame_mse_values = []\n",
        "    duration_ratios = []\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        # Get actual lengths for this sample\n",
        "        target_len = output_lengths[i]\n",
        "        \n",
        "        # Compute metrics on valid lengths\n",
        "        mel_output = mel_outputs[i, :, :target_len]\n",
        "        mel_target = mel_targets[i, :, :target_len]\n",
        "        \n",
        "        # MCD and Frame MSE for this sample\n",
        "        mcd = F.mse_loss(mel_output, mel_target, reduction='mean')\n",
        "        mcd_values.append(mcd.item())\n",
        "        \n",
        "        frame_mse = torch.mean((mel_output - mel_target) ** 2, dim=0).mean()\n",
        "        frame_mse_values.append(frame_mse.item())\n",
        "        \n",
        "        # Duration ratio for this sample\n",
        "        pred_duration = mel_output.size(-1)\n",
        "        target_duration = target_len.item()\n",
        "        duration_ratios.append(pred_duration / target_duration)\n",
        "    \n",
        "    # Average metrics across batch\n",
        "    metrics['mcd'] = np.mean(mcd_values)\n",
        "    metrics['frame_mse'] = np.mean(frame_mse_values)\n",
        "    metrics['duration_ratio'] = np.mean(duration_ratios)\n",
        "    \n",
        "    # Attention metrics\n",
        "    # Coverage: how much of input received attention\n",
        "    coverage = torch.sum(alignments > 0.5, dim=2).float() / alignments.size(2)\n",
        "    metrics['attention_coverage'] = coverage.mean().item()\n",
        "    \n",
        "    # Entropy: measure of attention sharpness\n",
        "    entropy = -torch.sum(alignments * torch.log(alignments + 1e-8), dim=2) / alignments.size(2)\n",
        "    metrics['attention_entropy'] = entropy.mean().item()\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def train_model(model, text_processor, train_loader, optimizer, num_epochs=100, checkpoint_path=None):\n",
        "    \"\"\"Train Tacotron2 model with checkpoint training loop\"\"\"\n",
        "    # Initialize training state\n",
        "    start_epoch = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    patience_counter = 0\n",
        "    max_patience = 10\n",
        "    \n",
        "    history = {\n",
        "        'train_losses': [],\n",
        "        'learning_rates': [],\n",
        "        'gradients': [],\n",
        "        'mcd_values': [],\n",
        "        'frame_mse_values': [],\n",
        "        'attention_coverage': [],\n",
        "        'attention_entropy': [],\n",
        "        'duration_ratios': []\n",
        "    }\n",
        "\n",
        "    # Load checkpoint if available\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        # Load model and optimizer state\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        print(checkpoint)\n",
        "\n",
        "        # Load traning state\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        \n",
        "        # Get best loss from metrics\n",
        "        if 'metrics' in checkpoint:\n",
        "            best_loss = checkpoint['metrics']['loss']\n",
        "            print(f\"Loaded best loss from metrics: {best_loss:.4f}\")\n",
        "\n",
        "        if 'history' in checkpoint:\n",
        "            history = checkpoint['history']\n",
        "        \n",
        "        print(f\"Checkpoint loaded. Resuming training from epoch: {start_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        model.train()\n",
        "        epoch_metrics = {\n",
        "            'losses': [],\n",
        "            'mcd': [],\n",
        "            'frame_mse': [],\n",
        "            'attention_coverage': [],\n",
        "            'attention_entropy': [],\n",
        "            'duration_ratios': [],\n",
        "            'gradients': []\n",
        "        }\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        for batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move batch to device\n",
        "            inputs, input_lengths, targets, max_len, output_lengths = batch\n",
        "            inputs = inputs.to(device)\n",
        "            input_lengths = input_lengths.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output_lengths = output_lengths.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model((inputs, input_lengths, targets, max_len, output_lengths))\n",
        "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments = outputs\n",
        "\n",
        "            # Calculate loss with L2 regularization\n",
        "            mel_loss = F.mse_loss(mel_outputs, targets) + F.mse_loss(mel_outputs_postnet, targets)\n",
        "            \n",
        "            # Gate loss for stop token prediction\n",
        "            gate_loss = F.binary_cross_entropy_with_logits(\n",
        "                gate_outputs, \n",
        "                torch.zeros_like(gate_outputs)\n",
        "            )\n",
        "            \n",
        "            # Total loss\n",
        "            l2_lambda = 1e-6\n",
        "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
        "            loss = mel_loss + gate_loss + l2_lambda * l2_norm\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            epoch_metrics['gradients'].append(grad_norm.item())\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute additional metrics\n",
        "            metrics = compute_metrics(\n",
        "                mel_outputs, \n",
        "                targets, \n",
        "                alignments,\n",
        "                input_lengths,  # From the batch\n",
        "                output_lengths  # From the batch\n",
        "            )\n",
        "            \n",
        "            # Store metrics\n",
        "            epoch_metrics['losses'].append(loss.item())\n",
        "            epoch_metrics['mcd'].append(metrics['mcd'])\n",
        "            epoch_metrics['frame_mse'].append(metrics['frame_mse'])\n",
        "            epoch_metrics['attention_coverage'].append(metrics['attention_coverage'])\n",
        "            epoch_metrics['attention_entropy'].append(metrics['attention_entropy'])\n",
        "            epoch_metrics['duration_ratios'].append(metrics['duration_ratio'])\n",
        "            \n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'mcd': f\"{metrics['mcd']:.4f}\",\n",
        "                'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
        "            })\n",
        "\n",
        "        # Calculate epoch averages\n",
        "        avg_metrics = {\n",
        "            'loss': np.mean(epoch_metrics['losses']),\n",
        "            'mcd': np.mean(epoch_metrics['mcd']),\n",
        "            'frame_mse': np.mean(epoch_metrics['frame_mse']),\n",
        "            'attention_coverage': np.mean(epoch_metrics['attention_coverage']),\n",
        "            'attention_entropy': np.mean(epoch_metrics['attention_entropy']),\n",
        "            'duration_ratio': np.mean(epoch_metrics['duration_ratios']),\n",
        "            'gradient': np.mean(epoch_metrics['gradients'])\n",
        "        }\n",
        "\n",
        "        # Update history\n",
        "        for key, value in avg_metrics.items():\n",
        "            if key + 's' in history:  # Add 's' for plural in history keys\n",
        "                history[key + 's'].append(value)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
        "        for metric, value in avg_metrics.items():\n",
        "            print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on MCD\n",
        "        scheduler.step(avg_metrics['mcd'])\n",
        "\n",
        "        # Save checkpoint if loss improved\n",
        "        if avg_metrics['loss'] < best_loss:\n",
        "            best_loss = avg_metrics['loss']\n",
        "            patience_counter = 0\n",
        "            \n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'metrics': avg_metrics,\n",
        "                'history': history,\n",
        "                'best_loss': best_loss\n",
        "            }, f'{output_path}/models/best_model.pt')\n",
        "            \n",
        "            print(\"\\nNew best model saved!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # For Debug Only\n",
        "        if epoch == 0:\n",
        "            generate_test_sample(model, text_processor, f\"{output_path}/samples/test_epoch_{epoch+1}.wav\")\n",
        "\n",
        "        # Regular checkpoint and evaluation\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            # Save checkpoint\n",
        "            checkpoint_path = f'{output_path}/models/checkpoint_epoch_{epoch+1}.pt'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'metrics': avg_metrics,\n",
        "                'history': history\n",
        "            }, checkpoint_path)\n",
        "\n",
        "            # Generate test samples\n",
        "            generate_test_sample(model, text_processor, f\"{output_path}/samples/test_epoch_{epoch+1}.wav\")\n",
        "            \n",
        "            # Plot training history\n",
        "            plot_training_history(history, epoch+1)\n",
        "            \n",
        "            # Plot attention alignments\n",
        "            plot_attention(alignments[0], f\"{output_path}/attention/attention_epoch_{epoch+1}.png\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= max_patience:\n",
        "            print(\"\\nEarly stopping triggered!\")\n",
        "            break\n",
        "\n",
        "def plot_training_history(history, epoch):\n",
        "    \"\"\"Enhanced training history visualization\"\"\"\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    \n",
        "    # Plot all metrics\n",
        "    metrics = [\n",
        "        ('train_losses', 'Training Loss'),\n",
        "        ('mcd_values', 'Mel-Cepstral Distortion'),\n",
        "        ('frame_mse_values', 'Frame MSE'),\n",
        "        ('attention_coverage', 'Attention Coverage'),\n",
        "        ('attention_entropy', 'Attention Entropy'),\n",
        "        ('duration_ratios', 'Duration Ratio'),\n",
        "        ('learning_rates', 'Learning Rate'),\n",
        "        ('gradients', 'Gradient Norm')\n",
        "    ]\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(metrics, 1):\n",
        "        plt.subplot(4, 2, idx)\n",
        "        plt.plot(history[metric])\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_path}/training_history_epoch_{epoch}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_attention(attention, filename):\n",
        "    \"\"\"Plot attention alignment\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(attention.detach().cpu().numpy(), aspect='auto', origin='lower')\n",
        "    plt.colorbar()\n",
        "    plt.title('Attention Alignment')\n",
        "    plt.xlabel('Decoder Steps')\n",
        "    plt.ylabel('Encoder Steps')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "def generate_test_sample(model, text_processor, output_file):\n",
        "    \"\"\"Generate test samples with error handling and audio saving\"\"\"\n",
        "    model.eval()\n",
        "    Path(output_file).parent.mkdir(exist_ok=True)\n",
        "    \n",
        "    test_texts = [\n",
        "        \"ខ្ញុំស្រឡាញ់អ្នក\",  # I love you\n",
        "        \"សួស្តី\",           # Hello\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        for i, test_text in enumerate(test_texts):\n",
        "            with torch.no_grad():\n",
        "                print(f\"\\nGenerating sample for: {test_text}\")\n",
        "                \n",
        "                # Process text\n",
        "                sequence = text_processor.process_text(test_text)\n",
        "                sequence = sequence.unsqueeze(0).to(device)\n",
        "                input_lengths = torch.LongTensor([sequence.size(1)]).to(device)\n",
        "\n",
        "                # Generate mel spectrogram\n",
        "                mel_outputs, mel_lengths, alignments = model.infer(sequence, input_lengths)\n",
        "                \n",
        "                # Create unique output paths\n",
        "                current_output_file = output_file.replace('.wav', f'_{i+1}.wav')\n",
        "                mel_plot_path = current_output_file.replace('.wav', '_mel.png')\n",
        "                attention_path = current_output_file.replace('.wav', '_attention.png')\n",
        "                \n",
        "                # Save mel spectrogram plot\n",
        "                plot_mel_spectrogram(mel_outputs[0], mel_plot_path, \n",
        "                                   title=f\"Mel Spectrogram: {test_text}\")\n",
        "                \n",
        "                # Move to CPU and convert to numpy\n",
        "                mel_spec = mel_outputs[0].cpu().numpy()\n",
        "                \n",
        "                # Create mel filterbank\n",
        "                mel_basis = librosa.filters.mel(\n",
        "                    sr=22050,\n",
        "                    n_fft=1024,\n",
        "                    n_mels=80,\n",
        "                    fmin=0,\n",
        "                    fmax=8000\n",
        "                )\n",
        "                \n",
        "                # Convert to linear spectrogram\n",
        "                linear_spec = np.maximum(1e-10, np.dot(mel_basis.T, mel_spec))\n",
        "                \n",
        "                # Generate audio using librosa's Griffin-Lim\n",
        "                audio_numpy = librosa.griffinlim(\n",
        "                    linear_spec,\n",
        "                    n_iter=64,\n",
        "                    hop_length=256,\n",
        "                    win_length=1024,\n",
        "                    window='hann'\n",
        "                )\n",
        "                \n",
        "                # Normalize audio\n",
        "                audio_numpy = audio_numpy / np.abs(audio_numpy).max()\n",
        "                \n",
        "                # Apply fade in/out\n",
        "                fade_length = int(0.01 * 22050)  # 10ms fade\n",
        "                fade_in = np.linspace(0, 1, fade_length)\n",
        "                fade_out = np.linspace(1, 0, fade_length)\n",
        "                \n",
        "                audio_numpy[:fade_length] *= fade_in\n",
        "                audio_numpy[-fade_length:] *= fade_out\n",
        "                \n",
        "                # Save audio\n",
        "                sf.write(current_output_file, audio_numpy, 22050)\n",
        "                \n",
        "                # Save attention plot\n",
        "                plot_attention(alignments[0], attention_path)\n",
        "                \n",
        "                print(f\"Generated: {current_output_file}\")\n",
        "                print(f\"Mel shape: {mel_outputs.shape}\")\n",
        "                print(f\"Audio length: {len(audio_numpy)/22050:.2f} seconds\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error generating test sample: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "    print(\"\\nTest sample generation complete!\")\n",
        "\n",
        "def plot_mel_spectrogram(mel_spectrogram, plot_path, title=\"Mel Spectrogram\"):\n",
        "    \"\"\"\n",
        "    Plot mel spectrogram with detailed visualization\n",
        "    \n",
        "    Args:\n",
        "        mel_spectrogram: Tensor containing the mel spectrogram\n",
        "        plot_path: Path to save the plot\n",
        "        title: Title for the plot\n",
        "    \"\"\"\n",
        "    # Convert to numpy array\n",
        "    mel_np = mel_spectrogram.cpu().numpy()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Create main spectrogram plot\n",
        "    plt.subplot(1, 1, 1)\n",
        "    im = plt.imshow(mel_np, \n",
        "                    aspect='auto', \n",
        "                    origin='lower',\n",
        "                    interpolation='none', \n",
        "                    cmap='viridis')\n",
        "    \n",
        "    # Add colorbar\n",
        "    plt.colorbar(im, ax=plt.gca())\n",
        "    \n",
        "    # Add labels and title\n",
        "    plt.xlabel('Frames')\n",
        "    plt.ylabel('Mel Channels')\n",
        "    plt.title(title)\n",
        "    \n",
        "    # Add grid\n",
        "    plt.grid(False)\n",
        "    \n",
        "    # Adjust layout and save\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"Mel spectrogram plot saved to: {plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O84puOB9aez"
      },
      "source": [
        "## 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy8oplV59cMq"
      },
      "source": [
        "### 4.1 Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kqkPrVDs9W63"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(model, test_loader):\n",
        "    \"\"\"Calculate evaluation metrics\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    mel_l1_loss = 0\n",
        "    mel_l2_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            text = batch['text'].to(device)\n",
        "            text_lengths = batch['text_lengths'].to(device)\n",
        "            mel_target = batch['mel_target'].to(device)\n",
        "\n",
        "            # Generate mel spectrograms\n",
        "            mel_output, mel_output_postnet, _, _ = model(\n",
        "                text, text_lengths, mel_target)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mel_l1_loss += nn.L1Loss()(mel_output_postnet, mel_target).item()\n",
        "            mel_l2_loss += nn.MSELoss()(mel_output_postnet, mel_target).item()\n",
        "\n",
        "    # Average metrics\n",
        "    mel_l1_loss /= len(test_loader)\n",
        "    mel_l2_loss /= len(test_loader)\n",
        "\n",
        "    return {\n",
        "        'mel_l1_loss': mel_l1_loss,\n",
        "        'mel_l2_loss': mel_l2_loss\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBchccw89lCU"
      },
      "source": [
        "### 4.2 Alignment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x-Rk4yWr9eh9"
      },
      "outputs": [],
      "source": [
        "def plot_alignment(alignment):\n",
        "    \"\"\"Plot attention alignment\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    im = ax.imshow(alignment.cpu().numpy(), aspect='auto', origin='lower')\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    ax.set_title('Alignment')\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deQxSgCA9q2h"
      },
      "source": [
        "## 5. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU5sPUpJ9scp"
      },
      "source": [
        "### 5.1 Text to Speech Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Zr9BxL4_9n3v"
      },
      "outputs": [],
      "source": [
        "def text_to_speech(text, model, vocoder, text_processor, output_path=None):\n",
        "    \"\"\"Complete TTS pipeline\"\"\"\n",
        "    model.eval()\n",
        "    vocoder.eval()\n",
        "\n",
        "    # Process text\n",
        "    sequence = text_processor.process_text(text)\n",
        "    sequence = sequence.unsqueeze(0).to(device)\n",
        "    sequence_length = torch.LongTensor([sequence.size(1)]).to(device)\n",
        "\n",
        "    # Generate mel spectrogram\n",
        "    with torch.no_grad():\n",
        "        mel_outputs, mel_outputs_postnet, _, alignments = model.infer(\n",
        "            sequence, sequence_length)\n",
        "\n",
        "        # Generate audio\n",
        "        audio = vocoder(mel_outputs_postnet)\n",
        "        audio = audio.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Save if path provided\n",
        "    if output_path:\n",
        "        sf.write(output_path, audio, 22050)\n",
        "\n",
        "    return audio, mel_outputs_postnet, alignments\n",
        "\n",
        "# Example usage\n",
        "def inference_demo():\n",
        "    # Load models\n",
        "    model = load_pretrained_model()\n",
        "    vocoder = load_vocoder()\n",
        "    text_processor = TextProcessor()\n",
        "\n",
        "    # Test text\n",
        "    khmer_text = \"ខ្ញុំស្រឡាញ់អ្នក\"  # \"I love you\" in Khmer\n",
        "\n",
        "    # Generate speech\n",
        "    audio, mel, alignment = text_to_speech(\n",
        "        khmer_text,\n",
        "        model,\n",
        "        vocoder,\n",
        "        text_processor,\n",
        "        \"output.wav\"\n",
        "    )\n",
        "\n",
        "    # Plot alignment\n",
        "    plot_alignment(alignment[0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvsoNPc090LK"
      },
      "source": [
        "### 5.2 Batch Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J2t6Wl5F918y"
      },
      "outputs": [],
      "source": [
        "def process_transcript(transcript_path, output_dir, model, vocoder, text_processor):\n",
        "    \"\"\"Process entire transcript file\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Load transcript\n",
        "    df = load_transcript(transcript_path)\n",
        "\n",
        "    # Process each entry\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
        "        output_path = output_dir / f\"{row['audio_file']}.wav\"\n",
        "\n",
        "        # Skip if already exists\n",
        "        if output_path.exists():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Generate speech\n",
        "            text_to_speech(\n",
        "                row['text'],\n",
        "                model,\n",
        "                vocoder,\n",
        "                text_processor,\n",
        "                output_path\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {row['audio_file']}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxFJuOqH973W"
      },
      "source": [
        "# Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0rDmwN-R93gL"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Define Path Variables\n",
        "    transcript_path = f\"{output_path}/line_index.tsv\"\n",
        "    audio_path = f\"{output_path}/wavs\"\n",
        "\n",
        "    # Load data\n",
        "    df = load_transcript(transcript_path)\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "\n",
        "    # Perform EDA\n",
        "    # durations, sample_rates, file_sizes = analyze_audio_files(df, audio_path)\n",
        "    # plot_audio_stats(durations, sample_rates, file_sizes)\n",
        "    # analyze_text(df)\n",
        "\n",
        "    # Initialize processors\n",
        "    text_processor = TextProcessor()\n",
        "    audio_processor = AudioProcessor()\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = AudioDataset(df, audio_path, text_processor, audio_processor)\n",
        "\n",
        "    # Parameters\n",
        "    train_params = {\n",
        "        'batch_size': 24,\n",
        "        'num_epochs': 300,\n",
        "        'learning_rate': 1e-3,\n",
        "        'num_workers': 0\n",
        "    }\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=train_params['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=train_params['num_workers'],\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    model = load_pretrained_model()\n",
        "    vocoder = load_vocoder()\n",
        "\n",
        "    # Train model\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=train_params['learning_rate'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_model(\n",
        "        model=model,\n",
        "        text_processor=text_processor,\n",
        "        train_loader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        num_epochs=train_params['num_epochs']\n",
        "    )\n",
        "\n",
        "    # # Training from previous checkpoint\n",
        "    # train_model(\n",
        "    #     model=model,\n",
        "    #     text_processor=text_processor,\n",
        "    #     train_loader=train_loader,\n",
        "    #     optimizer=optimizer,\n",
        "    #     num_epochs=train_params['num_epochs'],\n",
        "    #     checkpoint_path=f'{output_path}/models/checkpoint_epoch_100.pt'\n",
        "    # )\n",
        "\n",
        "    # Evaluate\n",
        "    # metrics = calculate_metrics(model, train_loader)\n",
        "    # print(\"Evaluation metrics:\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NS12WHk_-IUx",
        "outputId": "735598f5-e9a3-4cf7-c9cc-8fda50858d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 2906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\ADMIN/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n",
            "C:\\Users\\ADMIN/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "C:\\Users\\ADMIN/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "Using cache found in C:\\Users\\ADMIN/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n",
            "C:\\Users\\ADMIN/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\SpeechSynthesis\\Tacotron2\\waveglow\\entrypoints.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(ckpt_file)\n",
            "f:\\Anaconda\\envs\\khmer-tts\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "f:\\Anaconda\\envs\\khmer-tts\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75b41a15548d4064ab883afed5a9dcac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/300 Summary:\n",
            "Loss: 9.0566\n",
            "Mcd: 6.9636\n",
            "Frame Mse: 6.9636\n",
            "Attention Coverage: 0.0014\n",
            "Attention Entropy: 0.0297\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 5.1085\n",
            "\n",
            "New best model saved!\n",
            "\n",
            "Generating sample for: ខ្ញុំស្រឡាញ់អ្នក\n",
            "Warning! Reached max decoder steps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6017 (\\N{KHMER LETTER KHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6098 (\\N{KHMER SIGN COENG}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6025 (\\N{KHMER LETTER NYO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6075 (\\N{KHMER VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6086 (\\N{KHMER SIGN NIKAHIT}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6047 (\\N{KHMER LETTER SA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6042 (\\N{KHMER LETTER RO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6049 (\\N{KHMER LETTER LA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6070 (\\N{KHMER VOWEL SIGN AA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6091 (\\N{KHMER SIGN BANTOC}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6050 (\\N{KHMER LETTER QA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6035 (\\N{KHMER LETTER NO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6016 (\\N{KHMER LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6017 (\\N{KHMER LETTER KHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6098 (\\N{KHMER SIGN COENG}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6025 (\\N{KHMER LETTER NYO}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6075 (\\N{KHMER VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6086 (\\N{KHMER SIGN NIKAHIT}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6047 (\\N{KHMER LETTER SA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6042 (\\N{KHMER LETTER RO}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6049 (\\N{KHMER LETTER LA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6070 (\\N{KHMER VOWEL SIGN AA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6091 (\\N{KHMER SIGN BANTOC}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6050 (\\N{KHMER LETTER QA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6035 (\\N{KHMER LETTER NO}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6016 (\\N{KHMER LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mel spectrogram plot saved to: ../dataset/samples/test_epoch_1_1_mel.png\n",
            "Generated: ../dataset/samples/test_epoch_1_1.wav\n",
            "Mel shape: torch.Size([1, 80, 1000])\n",
            "Audio length: 11.60 seconds\n",
            "\n",
            "Generating sample for: សួស្តី\n",
            "Warning! Reached max decoder steps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6047 (\\N{KHMER LETTER SA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6077 (\\N{KHMER VOWEL SIGN UA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6098 (\\N{KHMER SIGN COENG}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6031 (\\N{KHMER LETTER TA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6072 (\\N{KHMER VOWEL SIGN II}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6047 (\\N{KHMER LETTER SA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6077 (\\N{KHMER VOWEL SIGN UA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6098 (\\N{KHMER SIGN COENG}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6031 (\\N{KHMER LETTER TA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6072 (\\N{KHMER VOWEL SIGN II}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mel spectrogram plot saved to: ../dataset/samples/test_epoch_1_2_mel.png\n",
            "Generated: ../dataset/samples/test_epoch_1_2.wav\n",
            "Mel shape: torch.Size([1, 80, 1000])\n",
            "Audio length: 11.60 seconds\n",
            "\n",
            "Test sample generation complete!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43fff32b495418ca345ced4cedd7854",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/300 Summary:\n",
            "Loss: 4.2772\n",
            "Mcd: 3.1843\n",
            "Frame Mse: 3.1843\n",
            "Attention Coverage: 0.0038\n",
            "Attention Entropy: 0.0231\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 2.1352\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79caaa52fe5c46abb757b3e099e8700d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/300 Summary:\n",
            "Loss: 3.3666\n",
            "Mcd: 2.5683\n",
            "Frame Mse: 2.5683\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0205\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.7724\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eea2eea81ac148f0b52c7d81e918bd25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/300 Summary:\n",
            "Loss: 2.9863\n",
            "Mcd: 2.3304\n",
            "Frame Mse: 2.3304\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0196\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.5815\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b6a43d8e2904e1d8f873ebb5f4358c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/300 Summary:\n",
            "Loss: 2.7691\n",
            "Mcd: 2.1988\n",
            "Frame Mse: 2.1988\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0187\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.4405\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19c00e2fdd5c475eaa0d3b34149c2393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 6/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/300 Summary:\n",
            "Loss: 2.5668\n",
            "Mcd: 2.0896\n",
            "Frame Mse: 2.0896\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0177\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.3575\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73759978bced46ddb0f1d679f99e8c73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 7/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/300 Summary:\n",
            "Loss: 2.5056\n",
            "Mcd: 2.0133\n",
            "Frame Mse: 2.0133\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0178\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.3854\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a8fb94bccd84158aaaccda4faadbffd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 8/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/300 Summary:\n",
            "Loss: 2.4085\n",
            "Mcd: 1.9586\n",
            "Frame Mse: 1.9586\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0170\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.4022\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ef281790eed4df7bcfbcee765c28cae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 9/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/300 Summary:\n",
            "Loss: 2.3906\n",
            "Mcd: 1.9104\n",
            "Frame Mse: 1.9104\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0170\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.2475\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8b11c2f7c3a4044a6c641b40368ca42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 10/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/300 Summary:\n",
            "Loss: 2.3095\n",
            "Mcd: 1.8664\n",
            "Frame Mse: 1.8664\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0166\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.1326\n",
            "\n",
            "New best model saved!\n",
            "\n",
            "Generating sample for: ខ្ញុំស្រឡាញ់អ្នក\n",
            "Warning! Reached max decoder steps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6017 (\\N{KHMER LETTER KHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6025 (\\N{KHMER LETTER NYO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6075 (\\N{KHMER VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6086 (\\N{KHMER SIGN NIKAHIT}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6042 (\\N{KHMER LETTER RO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6049 (\\N{KHMER LETTER LA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6070 (\\N{KHMER VOWEL SIGN AA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6091 (\\N{KHMER SIGN BANTOC}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6050 (\\N{KHMER LETTER QA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6035 (\\N{KHMER LETTER NO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:407: UserWarning: Glyph 6016 (\\N{KHMER LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6017 (\\N{KHMER LETTER KHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6025 (\\N{KHMER LETTER NYO}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6075 (\\N{KHMER VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6086 (\\N{KHMER SIGN NIKAHIT}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6042 (\\N{KHMER LETTER RO}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6049 (\\N{KHMER LETTER LA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6070 (\\N{KHMER VOWEL SIGN AA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6091 (\\N{KHMER SIGN BANTOC}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6050 (\\N{KHMER LETTER QA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6035 (\\N{KHMER LETTER NO}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
            "D:\\temp\\ipykernel_13668\\3541702747.py:408: UserWarning: Glyph 6016 (\\N{KHMER LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mel spectrogram plot saved to: ../dataset/samples/test_epoch_10_1_mel.png\n",
            "Generated: ../dataset/samples/test_epoch_10_1.wav\n",
            "Mel shape: torch.Size([1, 80, 1000])\n",
            "Audio length: 11.60 seconds\n",
            "\n",
            "Generating sample for: សួស្តី\n",
            "Mel spectrogram plot saved to: ../dataset/samples/test_epoch_10_2_mel.png\n",
            "Generated: ../dataset/samples/test_epoch_10_2.wav\n",
            "Mel shape: torch.Size([1, 80, 3])\n",
            "Audio length: 0.02 seconds\n",
            "\n",
            "Test sample generation complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Anaconda\\envs\\khmer-tts\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=512\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "637d0951d8c94e1abb8a7c17bc75cc54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 11/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11/300 Summary:\n",
            "Loss: 2.2388\n",
            "Mcd: 1.8252\n",
            "Frame Mse: 1.8252\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0159\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.1552\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12e327ee22b24fb18338c798db8d0005",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 12/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/300 Summary:\n",
            "Loss: 2.2490\n",
            "Mcd: 1.7984\n",
            "Frame Mse: 1.7984\n",
            "Attention Coverage: 0.0041\n",
            "Attention Entropy: 0.0165\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.1162\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45563c7dff08431190316fc6f07d30f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 13/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13/300 Summary:\n",
            "Loss: 2.1967\n",
            "Mcd: 1.7682\n",
            "Frame Mse: 1.7682\n",
            "Attention Coverage: 0.0041\n",
            "Attention Entropy: 0.0160\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.1463\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a41de61098ad439294b22024d3812867",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 14/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/300 Summary:\n",
            "Loss: 2.1766\n",
            "Mcd: 1.7403\n",
            "Frame Mse: 1.7403\n",
            "Attention Coverage: 0.0042\n",
            "Attention Entropy: 0.0159\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0837\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f28a2bcf01fb457f9be4dbccbeb4c1d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 15/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15/300 Summary:\n",
            "Loss: 2.1477\n",
            "Mcd: 1.7185\n",
            "Frame Mse: 1.7185\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0166\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0821\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58dbf5cfc8134df4bec8632fbb42698c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 16/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/300 Summary:\n",
            "Loss: 2.1123\n",
            "Mcd: 1.6925\n",
            "Frame Mse: 1.6925\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0164\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0909\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acfaeb78e9b245b1b182d6a4398dec2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 17/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/300 Summary:\n",
            "Loss: 2.1271\n",
            "Mcd: 1.6779\n",
            "Frame Mse: 1.6779\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0161\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0374\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e799f7a24d1f481d97de017e8f2a8c5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 18/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18/300 Summary:\n",
            "Loss: 2.0699\n",
            "Mcd: 1.6575\n",
            "Frame Mse: 1.6575\n",
            "Attention Coverage: 0.0041\n",
            "Attention Entropy: 0.0157\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0577\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6178789aec2548f2a8f1f772b192a22e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 19/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19/300 Summary:\n",
            "Loss: 2.0495\n",
            "Mcd: 1.6384\n",
            "Frame Mse: 1.6384\n",
            "Attention Coverage: 0.0041\n",
            "Attention Entropy: 0.0155\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 0.9804\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "face6487a7e34b20a5133669ca3bbd08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 20/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20/300 Summary:\n",
            "Loss: 2.0659\n",
            "Mcd: 1.6235\n",
            "Frame Mse: 1.6235\n",
            "Attention Coverage: 0.0041\n",
            "Attention Entropy: 0.0158\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0460\n",
            "\n",
            "Generating sample for: ខ្ញុំស្រឡាញ់អ្នក\n",
            "Warning! Reached max decoder steps\n",
            "Mel spectrogram plot saved to: ../dataset/samples/test_epoch_20_1_mel.png\n",
            "Generated: ../dataset/samples/test_epoch_20_1.wav\n",
            "Mel shape: torch.Size([1, 80, 1000])\n",
            "Audio length: 11.60 seconds\n",
            "\n",
            "Generating sample for: សួស្តី\n",
            "Warning! Reached max decoder steps\n",
            "Mel spectrogram plot saved to: ../dataset/samples/test_epoch_20_2_mel.png\n",
            "Generated: ../dataset/samples/test_epoch_20_2.wav\n",
            "Mel shape: torch.Size([1, 80, 1000])\n",
            "Audio length: 11.60 seconds\n",
            "\n",
            "Test sample generation complete!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da126e33d04740cdbe44bf9d81a855c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 21/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 21/300 Summary:\n",
            "Loss: 2.0057\n",
            "Mcd: 1.6115\n",
            "Frame Mse: 1.6115\n",
            "Attention Coverage: 0.0040\n",
            "Attention Entropy: 0.0148\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0919\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a462a83f536d4be78591c000d5daa628",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 22/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 22/300 Summary:\n",
            "Loss: 2.0577\n",
            "Mcd: 1.5962\n",
            "Frame Mse: 1.5962\n",
            "Attention Coverage: 0.0041\n",
            "Attention Entropy: 0.0156\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0740\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc938a7ee96e497e8d6f50a3be8bec1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 23/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 23/300 Summary:\n",
            "Loss: 1.9789\n",
            "Mcd: 1.5759\n",
            "Frame Mse: 1.5759\n",
            "Attention Coverage: 0.0042\n",
            "Attention Entropy: 0.0151\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 0.8974\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e4a80acd51349b596d640ca77691c5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 24/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 24/300 Summary:\n",
            "Loss: 1.9620\n",
            "Mcd: 1.5644\n",
            "Frame Mse: 1.5644\n",
            "Attention Coverage: 0.0042\n",
            "Attention Entropy: 0.0151\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 0.9682\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04d521f32cb14298abc02e0d176737f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 25/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 25/300 Summary:\n",
            "Loss: 1.9714\n",
            "Mcd: 1.5541\n",
            "Frame Mse: 1.5541\n",
            "Attention Coverage: 0.0041\n",
            "Attention Entropy: 0.0150\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 0.9771\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cff7e7d237aa40ed867207e01ef7a5fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 26/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 26/300 Summary:\n",
            "Loss: 1.9569\n",
            "Mcd: 1.5457\n",
            "Frame Mse: 1.5457\n",
            "Attention Coverage: 0.0043\n",
            "Attention Entropy: 0.0146\n",
            "Duration Ratio: 1.0000\n",
            "Gradient: 1.0375\n",
            "\n",
            "New best model saved!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b188710ab50247de854d0e33b00e86b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 27/300:   0%|          | 0/122 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilvznPQAJjp3"
      },
      "source": [
        "### Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQLNwr8pUJqp",
        "outputId": "56ce00e7-7168-46ce-8185-f93fd566e411"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-57-34b755bfa3b9>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Architecture:\n",
            "Tacotron2(\n",
            "  (embedding): Embedding(148, 512)\n",
            "  (encoder): Encoder(\n",
            "    (convolutions): ModuleList(\n",
            "      (0-2): 3 x Sequential(\n",
            "        (0): ConvNorm(\n",
            "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        )\n",
            "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (prenet): Prenet(\n",
            "      (layers): ModuleList(\n",
            "        (0): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
            "        )\n",
            "        (1): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (attention_rnn): LSTMCell(768, 1024)\n",
            "    (attention_layer): Attention(\n",
            "      (query_layer): LinearNorm(\n",
            "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
            "      )\n",
            "      (memory_layer): LinearNorm(\n",
            "        (linear_layer): Linear(in_features=512, out_features=128, bias=False)\n",
            "      )\n",
            "      (v): LinearNorm(\n",
            "        (linear_layer): Linear(in_features=128, out_features=1, bias=False)\n",
            "      )\n",
            "      (location_layer): LocationLayer(\n",
            "        (location_conv): ConvNorm(\n",
            "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
            "        )\n",
            "        (location_dense): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoder_rnn): LSTMCell(1536, 1024, bias=1)\n",
            "    (linear_projection): LinearNorm(\n",
            "      (linear_layer): Linear(in_features=1536, out_features=80, bias=True)\n",
            "    )\n",
            "    (gate_layer): LinearNorm(\n",
            "      (linear_layer): Linear(in_features=1536, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (postnet): Postnet(\n",
            "    (convolutions): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): ConvNorm(\n",
            "          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        )\n",
            "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1-3): 3 x Sequential(\n",
            "        (0): ConvNorm(\n",
            "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        )\n",
            "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): ConvNorm(\n",
            "          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        )\n",
            "        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "Total parameters: 28,193,153\n",
            "Trainable parameters: 28,193,153\n",
            "\n",
            "Checkpoint information:\n",
            "Epoch: 29\n",
            "Loss: 1.8610427174411837\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def print_model_info(checkpoint_path):\n",
        "    \"\"\"Print model information from checkpoint\"\"\"\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Load Tacotron2 model\n",
        "    model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub',\n",
        "                          'nvidia_tacotron2',\n",
        "                          model_math='fp16',\n",
        "                          pretrained=False)\n",
        "\n",
        "    # Load state dict from checkpoint\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Print training info from checkpoint\n",
        "    print(f\"\\nCheckpoint information:\")\n",
        "    print(f\"Epoch: {checkpoint['epoch']}\")\n",
        "    print(f\"Loss: {checkpoint['loss']}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Usage\n",
        "checkpoint_path = f\"{output_path}/models/checkpoint_epoch_100.pt\"\n",
        "model = print_model_info(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JJA7c8TXwHQ"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "khmer-tts",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
