{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer, ToktokTokenizer, TweetTokenizer\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = \"dataset/line_index.tsv\"\n",
    "data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"key\", \"unused\", \"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles: 2906\n",
      "Mean length of abstracts in characters: 50.40\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic Preprocessing\n",
    "# Count the number of unique articles\n",
    "num_unique_articles = data[\"key\"].nunique()\n",
    "\n",
    "# Calculate the mean length of abstracts in characters\n",
    "abstract_lengths = data[\"abstract\"].dropna().apply(len)\n",
    "mean_abstract_length = abstract_lengths.mean()\n",
    "\n",
    "print(f\"Number of unique articles: {num_unique_articles}\")\n",
    "print(f\"Mean length of abstracts in characters: {mean_abstract_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary: 4347\n",
      "Number of tokens using TreebankWordTokenizer: 26179\n",
      "Number of tokens using ToktokTokenizer: 26179\n",
      "Number of tokens using TweetTokenizer: 26179\n"
     ]
    }
   ],
   "source": [
    "# 2. Word-Level Preprocessing\n",
    "# Split the abstracts into lists of words\n",
    "word_lists = data[\"abstract\"].dropna().apply(lambda x: x.split())\n",
    "\n",
    "# Count the number of different words (unique vocabulary size)\n",
    "vocabulary = set(word for word_list in word_lists for word in word_list)\n",
    "num_unique_words = len(vocabulary)\n",
    "\n",
    "print(f\"Number of unique words in the vocabulary: {num_unique_words}\")\n",
    "\n",
    "# Tokenize using NLTK tokenizers\n",
    "tokenizers = {\n",
    "    \"TreebankWordTokenizer\": TreebankWordTokenizer(),\n",
    "    \"ToktokTokenizer\": ToktokTokenizer(),\n",
    "    \"TweetTokenizer\": TweetTokenizer()\n",
    "}\n",
    "\n",
    "# Compare tokenization results\n",
    "for tokenizer_name, tokenizer in tokenizers.items():\n",
    "    tokenized_words = data[\"abstract\"].dropna().apply(lambda x: tokenizer.tokenize(x))\n",
    "    num_tokens = tokenized_words.apply(len).sum()\n",
    "    print(f\"Number of tokens using {tokenizer_name}: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   key numbers  number_percentage\n",
      "0  khm_0308_0011865648      []                0.0\n",
      "1  khm_0308_0032157149      []                0.0\n",
      "2  khm_0308_0038959268      []                0.0\n",
      "3  khm_0308_0054635313      []                0.0\n",
      "4  khm_0308_0055735195      []                0.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Domain Specificity and Regex\n",
    "# Use regex to extract numbers (ints, floats, years, percentages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
